"""
PaperGen_Pro - PDF è§£ææ¨¡å— (V3 æœ€ç»ˆç‰ˆ)

å®Œæ•´çš„åŒé€šé“æ¶æ„ï¼š
1. æå–åŸç”Ÿæ— æŸæ’å›¾å¹¶å­˜ç›˜ï¼ˆ`ext`, `width`, `height`ï¼‰ï¼Œå¸¦ `caption_context`ã€‚
2. å¹¶è¡Œå…¨å±€å¤šæ¨¡æ€ OCRï¼ˆkimi-k2.5ï¼‰ï¼Œå°†æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬ã€æ’ç‰ˆã€è¡¨æ ¼ã€å…¬å¼ï¼ˆLaTeXï¼‰å®Œæ•´è¿˜åŸä¸º Markdownã€‚
"""

import os
import io
import re
import json
import base64
import threading
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

import fitz  # PyMuPDF
from PIL import Image
from openai import OpenAI

import config


def parse_pdf(file_stream: bytes, filename: str = "document.pdf") -> Dict:
    """
    è§£æ PDF æ–‡ä»¶ã€‚V3 ç‰ˆæœ¬å¯¹æ‰€æœ‰æ–‡ä»¶è¿›è¡Œï¼š
      1. å…¨é¡µæ¸²æŸ“
      2. æå–åŸç”Ÿå›¾ç‰‡
      3. å¹¶è¡Œå¤šçº¿ç¨‹å¤šæ¨¡æ€ OCR
      4. è½»åº¦æ¸…æ´—

    Returns:
        dict: {
            "text": str,           # Markdown + LaTeX æ•´æ–‡
            "is_scanned": bool,    # V3 ç»Ÿä¸€è§†ä¸º true è®©åç»­çŸ¥é“å·² OCR
            "images_data": list,   # åŸç”Ÿå›¾ç‰‡åˆ—è¡¨ï¼ˆå«æœ¬åœ°å­˜ç›˜è·¯å¾„ path å’Œ caption_contextï¼‰
        }
    """
    os.makedirs(config.TEMP_FIGURES_DIR, exist_ok=True)

    doc = fitz.open(stream=file_stream, filetype="pdf")
    page_count = len(doc)
    print(f"[PDF Parser V3] æ‰“å¼€æ–‡ä»¶: {filename}, å…± {page_count} é¡µ")

    # Step 1: æ‰«æå…¨é¡µæå–å›¾ç‰‡èµ„äº§
    images_data = _extract_native_images(doc, filename)
    print(f"[PDF Parser V3] æå–åˆ° {len(images_data)} å¼ åŸç”Ÿå›¾ç‰‡")

    # Step 2: å…¨é¡µæ¸²æŸ“
    # matrix=2 ç›¸å½“äº 144 DPIï¼Œå¹³è¡¡é€Ÿåº¦ä¸ç²¾åº¦
    matrix = fitz.Matrix(2, 2)
    pages_for_ocr = []
    
    print(f"[PDF Parser V3] å¼€å§‹æ¸²æŸ“ {page_count} é¡µç”¨äº OCR...")
    for i in range(page_count):
        pix = doc[i].get_pixmap(matrix=matrix)
        pages_for_ocr.append({"page": i + 1, "img_bytes": pix.tobytes("png")})

    doc.close()

    # Step 3: å¹¶å‘ OCR è°ƒç”¨
    print(f"[PDF Parser V3] å¼€å§‹ {config.MAX_WORKERS if hasattr(config, 'MAX_WORKERS') else 5} å¹¶å‘å¤šæ¨¡æ€ OCR...")
    ocr_results = _parallel_ocr_pages(pages_for_ocr)

    # åˆå¹¶ç»“æœ
    combined_md = "\n\n---\n\n".join(
        f"<!-- Page {r['page']} -->\n{r['markdown']}"
        if r['markdown'] else f"<!-- Page {r['page']} ERROR: {r['error']} -->"
        for r in ocr_results
    )

    # Step 4: æ¸…æ´—æ–‡æœ¬
    final_text = _clean_text(combined_md)
    print(f"[PDF Parser V3] OCR ä¸æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆæ–‡æœ¬é•¿åº¦: {len(final_text)}")

    # Step 5: åˆ©ç”¨ OCR å…¨æ–‡ä¸ºå›¾ç‰‡ä¸Šä¸‹æ–‡åšè¯­ä¹‰å¯ŒåŒ–ï¼ˆé›¶é¢å¤– API è°ƒç”¨ï¼‰
    _enrich_image_contexts(images_data, final_text)

    return {
        "text": final_text,
        "is_scanned": True, # å¯¹äºä¸‹æ¸¸æµç¨‹ç»Ÿä¸€ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å…¨éƒ¨é€šè¿‡ OCR å’Œ Markdown æå–äº†
        "images_data": images_data,
    }


def parse_multiple_pdfs(file_streams: List[Tuple[bytes, str]]) -> Dict:
    """
    è§£æå¤šä¸ª PDF æ–‡ä»¶ï¼Œåˆå¹¶ç»“æœä»¥ä¾›å†™ä½œå›¾è°±è°ƒç”¨ã€‚
    """
    all_text_parts = []
    all_images = []

    for idx, (stream, fname) in enumerate(file_streams):
        print(f"\n[PDF Parser V3] === è§£æç¬¬ {idx + 1}/{len(file_streams)} ä¸ªæ–‡ä»¶: {fname} ===")
        result = parse_pdf(stream, fname)
        all_text_parts.append(f"\n\n{'='*60}\nğŸ“„ æ–‡ä»¶: {fname}\n{'='*60}\n\n{result['text']}")
        all_images.extend(result["images_data"])

    return {
        "text": "\n".join(all_text_parts),
        "is_scanned": True,
        "images_data": all_images,
    }


def _extract_native_images(doc: fitz.Document, filename: str) -> List[Dict]:
    """æå–åŸç”Ÿä¿ç•™æ’å›¾ï¼Œå¹¶å­˜æ”¾åˆ°ä¸´æ—¶ç›®å½•ã€‚"""
    images_data = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        image_list = page.get_images(full=True)
        
        for img_idx, img_info in enumerate(image_list):
            xref = img_info[0]
            try:
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image.get("ext", "png")
                
                img_pil = Image.open(io.BytesIO(image_bytes))
                width, height = img_pil.width, img_pil.height
                
                if width < config.MIN_IMAGE_WIDTH or height < config.MIN_IMAGE_HEIGHT:
                    continue
                
                # å¯»æ‰¾ä¸Šä¸‹æ–‡
                caption_context = ""
                image_rects = page.get_image_rects(xref)
                if image_rects:
                    raw_context = _extract_caption_context(page, image_rects[0], config.CAPTION_CONTEXT_CHARS)
                    # æ¸…é™¤åŸè®ºæ–‡çš„å›¾ç‰‡æ ‡å· (ä¾‹å¦‚ "Figure 1:", "å›¾3(b) - ")ï¼Œä¿ç•™çº¯ç²¹çš„å›¾ç‰‡åå­—+ç‰‡æ®µ
                    # åŒ¹é…: å›¾/Figure/Fig. + æ•°å­— + å¯èƒ½çš„å­—æ¯ç¼–å·(å¦‚ 1a, 1(a), 1ï¼ˆbï¼‰) + å¯èƒ½çš„åˆ†éš”ç¬¦
                    pattern = r'(?:å›¾|Figure|Fig\.?)\s*\d+(?:[a-zA-Z]|\([a-zA-Z]\)|ï¼ˆ[a-zA-Z]ï¼‰)*[\s:ï¼š\.\-]*'
                    caption_context = re.sub(pattern, '', raw_context, flags=re.IGNORECASE).strip()

                # å­˜å…¥ç‰©ç†ç£ç›˜ï¼ˆä¾›ä¸‹æ¸¸ doc_writer ä½¿ç”¨ï¼‰
                safe_name = filename.replace(" ", "_").replace(".", "_")
                img_filename = f"{safe_name}_page{page_num + 1}_img{img_idx + 1}.{image_ext}"
                img_path = os.path.join(config.TEMP_FIGURES_DIR, img_filename)

                with open(img_path, "wb") as f:
                    f.write(image_bytes)

                images_data.append({
                    "path": img_path,
                    "page": page_num + 1,
                    "caption_context": caption_context,
                    "source_file": filename,
                    "size": f"{width}x{height}",
                })
            except Exception as e:
                print(f"[PDF Parser V3] æå–å›¾ç‰‡å¼‚å¸¸ P{page_num+1}_{img_idx+1}: {e}")
                continue
                
    return images_data


def _extract_caption_context(page: fitz.Page, image_bbox: fitz.Rect, context_chars: int) -> str:
    """æå–å›¾ç‰‡å‘¨å›´çš„æ–‡æœ¬ä½œä¸º caption ä¸Šä¸‹æ–‡"""
    try:
        page_text = page.get_text("text").strip()
        if not page_text:
            return ""
            
        words = page.get_text("words")
        if not words:
            return ""
            
        min_dist = float('inf')
        closest_word_idx = -1
        
        for i, w in enumerate(words):
            w_rect = fitz.Rect(w[:4])
            dx = max(0, w_rect.x0 - image_bbox.x1) + max(0, image_bbox.x0 - w_rect.x1)
            dy = max(0, w_rect.y0 - image_bbox.y1) + max(0, image_bbox.y0 - w_rect.y1)
            dist = (dx**2 + dy**2)**0.5
            
            if dist < min_dist:
                min_dist = dist
                closest_word_idx = i
                
        if closest_word_idx == -1:
            return ""
            
        start_idx = max(0, closest_word_idx - 50)
        end_idx = min(len(words), closest_word_idx + 50)
        
        context_words = words[start_idx:end_idx]
        context_text = " ".join([w[4] for w in context_words])
        
        if len(context_text) > context_chars * 2:
            center = len(context_text) // 2
            half = context_chars
            context_text = context_text[max(0, center-half):min(len(context_text), center+half)]
            
        return context_text.strip()
    except Exception:
        return ""


def _ocr_one_page(page_info: Dict) -> Dict:
    """å†…éƒ¨å•é¡µ OCR çº¿ç¨‹å‡½æ•°"""
    b64 = base64.b64encode(page_info["img_bytes"]).decode("utf-8")
    client = OpenAI(
        api_key=config.MULTIMODAL_API_KEY, 
        base_url=config.MULTIMODAL_API_BASE
    )
    
    # è·å–éœ€è¦ä½¿ç”¨çš„æ¨¡å‹
    model_name = getattr(config, "MULTIMODAL_MODEL_NAME", "kimi-k2.5")
    # Kimi è¦æ±‚ 1.0ï¼Œå…¶ä»–å¯ä»¥ 0.1
    temp = 1.0 if "kimi" in model_name else 0.1
    
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ PDF è§£æå™¨å’Œæ’ç‰ˆè¿˜åŸåŠ©æ‰‹ã€‚\n"
        "ä»»åŠ¡ï¼šç²¾ç¡®è¯†åˆ«æä¾›çš„æ–‡æ¡£å›¾ç‰‡ä¸­çš„æ‰€æœ‰å†…å®¹ï¼Œå¹¶å®Œæ•´åœ°è¿˜åŸä¸º Markdown æ ¼å¼è¾“å‡ºã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. çº¯æ–‡æœ¬å†…å®¹ä¿æŒåŸæ ·çš„æ®µè½ç»“æ„ã€‚\n"
        "2. æ‰€æœ‰æ•°å­¦å…¬å¼ï¼ˆè¡Œå†…æˆ–ç‹¬ç«‹å…¬å¼ï¼‰ä½¿ç”¨ LaTeX è¯­æ³•ï¼ŒåŒ…è£¹åœ¨ `$` æˆ– `$$` ä¸­è¾“å‡ºã€‚\n"
        "3. è¡¨æ ¼ä½¿ç”¨ Markdown è¡¨æ ¼è¯­æ³•è¿˜åŸã€‚\n"
        "4. å¿½ç•¥é¡µçœ‰ã€é¡µè„šã€æ— å…³é¡µç ï¼Œåªè¾“å‡ºæ­£æ–‡ã€å›¾è¡¨é¢˜æ³¨åŠå…¬å¼ã€‚\n"
        "5. ä¸è¦è¾“å‡ºä»»ä½•å¤šä½™çš„å¼€å¤´æˆ–ç»“å°¾å¯’æš„è¯­ï¼Œç›´æ¥è¿”å› Markdown æ–‡æœ¬å³å¯ã€‚"
    )
    
    try:
        resp = client.chat.completions.create(
            model=model_name,
            temperature=temp,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64}"}},
                    {"type": "text", "text": "è¯·ç²¾ç¡®è¯†åˆ«è¯¥é¡µé¢ä¸º Markdown"},
                ]},
            ],
        )
        md = resp.choices[0].message.content.strip()
        return {"page": page_info["page"], "markdown": md, "error": None}
    except Exception as e:
        return {"page": page_info["page"], "markdown": "", "error": str(e)}


def _parallel_ocr_pages(pages_for_ocr: List[Dict]) -> List[Dict]:
    """å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œ OCR"""
    total = len(pages_for_ocr)
    results = [None] * total
    max_workers = getattr(config, "MAX_WORKERS", 5)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {
            executor.submit(_ocr_one_page, p): i
            for i, p in enumerate(pages_for_ocr)
        }
        
        done = 0
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                res = future.result()
            except Exception as e:
                res = {"page": pages_for_ocr[idx]["page"], "markdown": "", "error": str(e)}
            
            results[idx] = res
            done += 1
            if done % max_workers == 0 or done == total:
                print(f"[PDF Parser V3] å¹¶å‘ OCR è¿›åº¦: {done}/{total} é¡µ")
                
    return results


def _clean_text(raw: str) -> str:
    """å»å™ªæ¸…æ´—"""
    cleaned = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f\ufeff\u200b\u200c\u200d\ufff0-\uffff]", "", raw)
    cleaned = re.sub(r"\n{4,}", "\n\n\n", cleaned)
    return cleaned.strip()


def _enrich_image_contexts(images_data: List[Dict], full_text: str) -> None:
    """
    åˆ©ç”¨å·²æœ‰çš„ OCR Markdown å…¨æ–‡ï¼Œä¸ºæ¯å¼ å›¾ç‰‡çš„ä¸Šä¸‹æ–‡åšè¯­ä¹‰å¯ŒåŒ–ã€‚
    
    åŸç†ï¼šå›¾ç‰‡çš„çŸ­ caption_context (ç”±ç‰©ç†è·ç¦»æŠ“å–) å¾€å¾€å¹²ç˜ªï¼Œ
    ä½† OCR å…¨æ–‡ä¸­å‡ ä¹ä¸€å®šåŒ…å«äº†å¯¹è¯¥å›¾ç‰‡æ›´è¯¦ç»†çš„æè¿°æ®µè½ã€‚
    æˆ‘ä»¬ç”¨ caption ä¸­çš„å…³é”®è¯åšæ¨¡ç³Šé”šç‚¹åŒ¹é…ï¼Œç²¾å‡†æˆªå–è¿™æ®µå¯Œæ–‡æœ¬ã€‚
    
    é›¶é¢å¤– API è°ƒç”¨ â€”â€” çº¯å­—ç¬¦ä¸²æ“ä½œï¼Œè€—æ—¶ < 50msã€‚
    """
    if not images_data or not full_text:
        return
    
    # å°†å…¨æ–‡åˆ‡æˆæ®µè½ï¼ˆæŒ‰åŒæ¢è¡Œåˆ†å‰²ï¼‰ï¼Œæ¯ä¸ªæ®µè½ä½œä¸ºåŒ¹é…å€™é€‰
    paragraphs = [p.strip() for p in re.split(r'\n{2,}', full_text) if p.strip() and len(p.strip()) > 20]
    
    if not paragraphs:
        print("[PDF Parser V3] å…¨æ–‡æ®µè½ä¸ºç©ºï¼Œè·³è¿‡å›¾ç‰‡ä¸Šä¸‹æ–‡å¯ŒåŒ–")
        return
    
    enriched_count = 0
    
    for img in images_data:
        caption = img.get("caption_context", "")
        if not caption or len(caption) < 5:
            continue
        
        # æå– caption ä¸­çš„å…³é”®è¯ï¼ˆå»æ‰å¸¸è§åœç”¨è¯å’ŒçŸ­è¯ï¼‰
        # ç”¨ä¸­æ–‡æŒ‰å­—ç¬¦åˆ‡å’Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ‡çš„æ··åˆæ–¹å¼
        keywords = set()
        for word in re.findall(r'[\u4e00-\u9fff]{2,}|[a-zA-Z]{3,}', caption):
            keywords.add(word.lower())
        
        if not keywords:
            continue
        
        # åœ¨å…¨æ–‡æ®µè½ä¸­æ‰¾"å‘½ä¸­å…³é”®è¯æœ€å¤š"çš„æ®µè½
        best_para_idx = -1
        best_score = 0
        
        for pidx, para in enumerate(paragraphs):
            para_lower = para.lower()
            score = sum(1 for kw in keywords if kw in para_lower)
            if score > best_score:
                best_score = score
                best_para_idx = pidx
        
        if best_para_idx >= 0 and best_score >= 2:
            # ä»¥æœ€ä½³æ®µè½ä¸ºä¸­å¿ƒï¼Œå–å‰å1æ®µï¼Œæ‹¼æ¥æˆ ~500 å­—çš„å¯Œä¸Šä¸‹æ–‡
            start = max(0, best_para_idx - 1)
            end = min(len(paragraphs), best_para_idx + 2)
            rich_context = "\n".join(paragraphs[start:end])
            
            # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¿‡é•¿
            if len(rich_context) > 800:
                rich_context = rich_context[:800]
            
            img["rich_context"] = rich_context
            enriched_count += 1
        else:
            # å›é€€ï¼šç”¨åŸå§‹ caption ä½œä¸º rich_context
            img["rich_context"] = caption
    
    print(f"[PDF Parser V3] å›¾ç‰‡ä¸Šä¸‹æ–‡å¯ŒåŒ–å®Œæˆ: {enriched_count}/{len(images_data)} å¼ å›¾ç‰‡æˆåŠŸåŒ¹é…åˆ° OCR å…¨æ–‡æ®µè½")

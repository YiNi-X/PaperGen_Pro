"""
============================================================
PaperGen_Pro - PDF å¤„ç†ç®¡é“å¯è§†åŒ–è°ƒè¯• (Streamlit ç‰ˆ V3 æœ€ç»ˆç‰ˆ)
============================================================

ç®¡é“æ­¥éª¤ï¼š
    Step 1: åŠ è½½ PDF + å…ƒæ•°æ®æå–
    Step 2: å…¨é¡µæ¸²æŸ“ä¸º PNGï¼ˆå« thumbnail é¢„è§ˆï¼‰
    Step 3: æå–åŸç”Ÿå›¾ç‰‡èµ„äº§ï¼ˆä¿ç•™åŸå§‹æ— æŸæ’å›¾ï¼‰
    Step 4: å¹¶è¡Œå¤šæ¨¡æ€ OCRï¼ˆThreadPoolExecutor + kimi-k2.5ï¼‰
             â†’ æ¯é¡µè¾“å‡ºå®Œæ•´ Markdownï¼ˆå« LaTeX å…¬å¼/è¡¨æ ¼ï¼‰
    Step 5: æ–‡æœ¬æ¸…æ´— + å…¬å¼ç»Ÿè®¡
    Step 6: æ–‡æœ¬åˆ†å—

è¿è¡Œæ–¹å¼ï¼š
    streamlit run function_test/pdf_put_test/test_pdf_pipeline.py

ä¾èµ–ï¼š
    pip install pymupdf streamlit Pillow openai python-dotenv
============================================================
"""

import os
import io
import re
import json
import base64
import tempfile
import threading
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

import fitz          # PyMuPDF
from PIL import Image
from openai import OpenAI
from dotenv import load_dotenv
import streamlit as st

# â”€â”€ åŠ è½½ .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
load_dotenv(os.path.join(_ROOT, ".env"))

# â”€â”€ å…¨å±€é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHUNK_SIZE              = 1000
SCANNED_TEXT_THRESHOLD  = 50
MIN_IMAGE_WIDTH         = 100
MIN_IMAGE_HEIGHT        = 100
CAPTION_CONTEXT_CHARS   = 300
API_BASE                = "https://api.moonshot.cn/v1"
OCR_MODEL               = "kimi-k2.5"
OCR_TEMPERATURE         = 1.0   # kimi-k2.5 only accepts T=1.0
DEFAULT_DPI             = 150   # matrix(2,2) â†’ 144 dpi effective
DEFAULT_WORKERS         = 5

# OCR Prompt
_OCR_SYSTEM = (
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ PDF è§£æå™¨å’Œæ’ç‰ˆè¿˜åŸåŠ©æ‰‹ã€‚\n"
    "ä»»åŠ¡ï¼šç²¾ç¡®è¯†åˆ«æä¾›çš„æ–‡æ¡£å›¾ç‰‡ä¸­çš„æ‰€æœ‰å†…å®¹ï¼Œå¹¶å®Œæ•´åœ°è¿˜åŸä¸º Markdown æ ¼å¼è¾“å‡ºã€‚\n"
    "è¦æ±‚ï¼š\n"
    "1. çº¯æ–‡æœ¬å†…å®¹ä¿æŒåŸæ ·çš„æ®µè½ç»“æ„ã€‚\n"
    "2. æ‰€æœ‰æ•°å­¦å…¬å¼ï¼ˆè¡Œå†…æˆ–ç‹¬ç«‹å…¬å¼ï¼‰ä½¿ç”¨ LaTeX è¯­æ³•ï¼ŒåŒ…è£¹åœ¨ `$` æˆ– `$$` ä¸­è¾“å‡ºã€‚\n"
    "3. è¡¨æ ¼ä½¿ç”¨ Markdown è¡¨æ ¼è¯­æ³•è¿˜åŸã€‚\n"
    "4. å¿½ç•¥é¡µçœ‰ã€é¡µè„šã€æ— å…³é¡µç ï¼Œåªè¾“å‡ºæ­£æ–‡ã€å›¾è¡¨é¢˜æ³¨åŠå…¬å¼ã€‚\n"
    "5. ä¸è¦è¾“å‡ºä»»ä½•å¤šä½™çš„å¼€å¤´æˆ–ç»“å°¾å¯’æš„è¯­ï¼Œç›´æ¥è¿”å› Markdown æ–‡æœ¬å³å¯ã€‚"
)


# ============================================================
#  STEP 1: load_pdf
# ============================================================
def load_pdf(file_path: str, filename: str) -> Optional[Tuple]:
    """åŠ è½½ PDF å¹¶æå–åŸºç¡€å…ƒæ•°æ®ã€‚"""
    try:
        doc = fitz.open(file_path)
    except Exception as e:
        st.error(f"âŒ æ— æ³•æ‰“å¼€ PDFï¼š{e}")
        return None

    meta        = doc.metadata
    page_count  = len(doc)
    size_bytes  = os.path.getsize(file_path)
    size_kb     = size_bytes / 1024

    first_text  = doc[0].get_text("text").strip() if page_count > 0 else ""
    is_scanned  = len(first_text) < SCANNED_TEXT_THRESHOLD

    meta_report = {
        "filename"             : filename,
        "file_size_bytes"      : size_bytes,
        "file_size_kb"         : round(size_kb, 2),
        "page_count"           : page_count,
        "is_scanned_preview"   : is_scanned,
        "first_page_char_count": len(first_text),
        "metadata"             : meta,
    }
    return doc, meta_report, is_scanned


# ============================================================
#  STEP 2: render_pages
# ============================================================
def render_pages(doc: fitz.Document, dpi: int = DEFAULT_DPI) -> List[Dict]:
    """
    æ¸²æŸ“æ¯é¡µä¸º PNG å­—èŠ‚ï¼Œé™„å¸¦ç¼©ç•¥å›¾ã€‚
    è¿”å› [{"page": int, "img_bytes": bytes, "thumb": PIL.Image}]
    """
    scale  = dpi / 72.0   # 72 dpi æ˜¯ PyMuPDF çš„åŸºç¡€ DPI
    matrix = fitz.Matrix(scale, scale)
    pages  = []
    for i in range(len(doc)):
        pix       = doc[i].get_pixmap(matrix=matrix)
        img_bytes = pix.tobytes("png")
        # ç¼©ç•¥å›¾ (max 300px å®½)
        pil       = Image.open(io.BytesIO(img_bytes))
        thumb_w   = min(300, pil.width)
        thumb_h   = int(pil.height * thumb_w / pil.width)
        thumb     = pil.resize((thumb_w, thumb_h), Image.LANCZOS)
        pages.append({"page": i + 1, "img_bytes": img_bytes, "thumb": thumb})
    return pages


def _extract_caption_context(page: fitz.Page, image_bbox: fitz.Rect, context_chars: int = CAPTION_CONTEXT_CHARS) -> str:
    """æå–å›¾ç‰‡å‘¨å›´çš„æ–‡æœ¬ä½œä¸º caption ä¸Šä¸‹æ–‡"""
    try:
        page_text = page.get_text("text").strip()
        if not page_text:
            return ""
            
        words = page.get_text("words")
        if not words:
            return ""
            
        # å¯»æ‰¾è·ç¦»å›¾ç‰‡ bbox æœ€è¿‘çš„è¯ä½œä¸ºåŸºå‡†ç‚¹
        min_dist = float('inf')
        closest_word_idx = -1
        
        for i, w in enumerate(words):
            w_rect = fitz.Rect(w[:4])
            
            # è®¡ç®—å•è¯çŸ©å½¢å’Œå›¾ç‰‡çŸ©å½¢çš„ä¸­å¿ƒç‚¹è·ç¦»
            dx = max(0, w_rect.x0 - image_bbox.x1) + max(0, image_bbox.x0 - w_rect.x1)
            dy = max(0, w_rect.y0 - image_bbox.y1) + max(0, image_bbox.y0 - w_rect.y1)
            dist = (dx**2 + dy**2)**0.5
            
            if dist < min_dist:
                min_dist = dist
                closest_word_idx = i
                
        if closest_word_idx == -1:
            return ""
            
        # æå–å‰åä¸Šä¸‹æ–‡
        start_idx = max(0, closest_word_idx - 50)  # å‘å‰å–çº¦50ä¸ªè¯
        end_idx = min(len(words), closest_word_idx + 50)  # å‘åå–çº¦50ä¸ªè¯
        
        context_words = words[start_idx:end_idx]
        context_text = " ".join([w[4] for w in context_words])
        
        # æˆªæ–­åˆ°æŒ‡å®šå­—ç¬¦æ•°
        if len(context_text) > context_chars * 2:
            center = len(context_text) // 2
            half = context_chars
            context_text = context_text[max(0, center-half):min(len(context_text), center+half)]
            
        return context_text.strip()
    except Exception as e:
        return f"[æå–ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}]"


# ============================================================
#  STEP 3: extract_images
# ============================================================
def extract_images_from_pdf(doc: fitz.Document, filename: str) -> List[Dict]:
    """æå–é¡µé¢ä¸­çš„åŸç”Ÿæ— æŸå›¾ç‰‡èµ„äº§åŠä¸Šä¸‹æ–‡ã€‚"""
    images_data = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        image_list = page.get_images(full=True)
        
        for img_idx, img_info in enumerate(image_list):
            xref = img_info[0]
            try:
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]
                
                img_pil = Image.open(io.BytesIO(image_bytes))
                width, height = img_pil.width, img_pil.height
                
                if width < MIN_IMAGE_WIDTH or height < MIN_IMAGE_HEIGHT:
                    continue
                
                # è·å–å›¾ç‰‡åœ¨é¡µé¢ä¸Šçš„è¾¹ç•Œæ¡†ä»¥æå–ä¸Šä¸‹æ–‡
                image_rects = page.get_image_rects(xref)
                caption_context = ""
                if image_rects:
                    # é€šå¸¸å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç©ºé—´ä½ç½®
                    caption_context = _extract_caption_context(page, image_rects[0], CAPTION_CONTEXT_CHARS)
                
                images_data.append({
                    "image_bytes": image_bytes,
                    "ext": image_ext,
                    "width": width,
                    "height": height,
                    "page": page_num + 1,
                    "img_index": img_idx + 1,
                    "caption_context": caption_context,
                    "source_file": filename,
                    "size_label": f"{width}Ã—{height}",
                })
            except Exception:
                continue
                
    return images_data


# ============================================================
#  STEP 4: parallel_ocr
# ============================================================
def _ocr_one_page(page_info: Dict, api_key: str) -> Dict:
    """å•é¡µ OCRï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰ã€‚"""
    b64     = base64.b64encode(page_info["img_bytes"]).decode("utf-8")
    client  = OpenAI(api_key=api_key, base_url=API_BASE)
    try:
        resp = client.chat.completions.create(
            model       = OCR_MODEL,
            temperature = OCR_TEMPERATURE,
            messages    = [
                {"role": "system", "content": _OCR_SYSTEM},
                {"role": "user", "content": [
                    {"type": "image_url",
                     "image_url": {"url": f"data:image/png;base64,{b64}"}},
                    {"type": "text",
                     "text": "è¯·å°†è¯¥é¡µé¢è½¬æ¢ä¸ºå¸¦å…¬å¼çš„ Markdownã€‚"},
                ]},
            ],
        )
        md = resp.choices[0].message.content.strip()
        return {"page": page_info["page"], "markdown": md, "error": None}
    except Exception as e:
        return {"page": page_info["page"], "markdown": "", "error": str(e)}


def parallel_ocr(
    pages: List[Dict],
    api_key: str,
    max_workers: int = DEFAULT_WORKERS,
) -> List[Dict]:
    """å¹¶è¡Œè°ƒç”¨ OCR APIã€‚è¿”å›æŒ‰é¡µç æ’åºçš„ç»“æœåˆ—è¡¨ã€‚"""
    total   = len(pages)
    results = [None] * total
    lock    = threading.Lock()

    progress_bar   = st.progress(0, text="â³ å¹¶è¡Œ OCR è¿›è¡Œä¸­...")
    status_area    = st.empty()
    done_count     = [0]

    page_cols = st.columns(min(3, total))

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {
            executor.submit(_ocr_one_page, p, api_key): i
            for i, p in enumerate(pages)
        }

        for future in as_completed(future_to_idx):
            idx    = future_to_idx[future]
            result = future.result()
            results[idx] = result

            with lock:
                done_count[0] += 1
                done = done_count[0]

            progress_bar.progress(done / total, text=f"â³ å·²å®Œæˆ {done}/{total} é¡µâ€¦")

            col_idx = (result["page"] - 1) % len(page_cols)
            with page_cols[col_idx]:
                if result["error"]:
                    st.error(f"ç¬¬ {result['page']} é¡µ âŒ {result['error'][:60]}")
                else:
                    st.caption(f"âœ… ç¬¬ {result['page']} é¡µ")
                    preview = result["markdown"][:300].replace("\n", " ")
                    st.code(preview, language=None)

    progress_bar.progress(1.0, text="âœ… å…¨éƒ¨é¡µé¢ OCR å®Œæˆï¼")
    status_area.empty()
    return results


# ============================================================
#  STEP 5: clean_text
# ============================================================
def clean_text(raw: str) -> Tuple[str, Dict]:
    original_len = len(raw)
    cleaned = raw

    cleaned = re.sub(
        r"[\x00-\x08\x0b\x0c\x0e-\x1f\ufeff\u200b\u200c\u200d\ufff0-\uffff]",
        "", cleaned
    )
    cleaned = re.sub(r"\n{4,}", "\n\n\n", cleaned)
    cleaned = cleaned.strip()

    cleaned_len = len(cleaned)
    reduction   = original_len - cleaned_len

    block_formula_count  = len(re.findall(r"\$\$[\s\S]+?\$\$", cleaned))
    inline_formula_count = len(re.findall(r"(?<!\$)\$(?!\$).+?(?<!\$)\$(?!\$)", cleaned))

    stats = {
        "original_chars"      : original_len,
        "cleaned_chars"       : cleaned_len,
        "reduced_chars"       : reduction,
        "compression_rate"    : f"{reduction / max(original_len, 1) * 100:.1f}%",
        "block_formulas"      : block_formula_count,
        "inline_formulas"     : inline_formula_count,
    }
    return cleaned, stats


# ============================================================
#  STEP 6: chunk_text
# ============================================================
def chunk_text(cleaned_text: str, chunk_size: int = CHUNK_SIZE) -> Tuple[List[Dict], int]:
    paragraphs = [p.strip() for p in cleaned_text.split("\n\n") if p.strip()]
    chunks, buf, buf_len = [], [], 0

    for para in paragraphs:
        pl = len(para)
        if pl > chunk_size:
            if buf:
                content = "\n\n".join(buf)
                chunks.append({"index": len(chunks) + 1, "char_count": len(content), "content": content})
                buf, buf_len = [], 0
            for i in range(0, pl, chunk_size):
                sub = para[i:i + chunk_size]
                chunks.append({"index": len(chunks) + 1, "char_count": len(sub), "content": sub})
        elif buf_len + pl > chunk_size:
            content = "\n\n".join(buf)
            chunks.append({"index": len(chunks) + 1, "char_count": len(content), "content": content})
            buf, buf_len = [para], pl
        else:
            buf.append(para)
            buf_len += pl

    if buf:
        content = "\n\n".join(buf)
        chunks.append({"index": len(chunks) + 1, "char_count": len(content), "content": content})

    return chunks, len(paragraphs)


# ============================================================
#  Streamlit ä¸»ç•Œé¢
# ============================================================
def main():
    st.set_page_config(
        page_title="PDF å¤šæ¨¡æ€ç®¡é“ (å®Œå…¨ä½“ V3)",
        page_icon="ğŸ§ ",
        layout="wide",
    )

    st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
    .main .block-container { padding-top: 2rem; max-width: 1200px; }
    h1 { font-family: 'Inter', sans-serif; }
    .step-card {
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
        border: 1px solid rgba(148,163,184,.15);
        border-radius: 14px;
        padding: 1.4rem 1.6rem;
        margin-bottom: 1rem;
    }
    .step-title {
        font-size: 1.1rem; font-weight: 700; color: #e2e8f0;
    }
    .badge {
        display:inline-block; border-radius:999px;
        font-size:.78rem; font-weight:600; padding:.18rem .65rem;
        margin-left:.4rem;
    }
    .badge-ocr  { background:#6366f1; color:#fff; }
    .badge-ok   { background:#22c55e; color:#fff; }
    .badge-warn { background:#f59e0b; color:#1a1a2e; }
    </style>
    """, unsafe_allow_html=True)

    with st.sidebar:
        st.markdown("## âš™ï¸ é…ç½®")
        api_key = st.text_input(
            "Moonshot API Key",
            value=os.environ.get("MOONSHOT_API_KEY", ""),
            type="password",
        )
        render_dpi = st.select_slider(
            "æ¸²æŸ“ DPI",
            options=[72, 100, 150, 200, 300],
            value=150,
        )
        max_workers = st.slider("æœ€å¤§å¹¶å‘æ•°", min_value=1, max_value=10, value=5)

    st.markdown("# ğŸ§  PDF å¤šæ¨¡æ€ OCR ç®¡é“ Â· å®Œå…¨ä½“ V3")
    st.caption("å…¨é¡µæ¸²æŸ“ â†’ åŸç”Ÿæ— æŸå›¾ç‰‡æŠ“å– â†’ å¹¶è¡Œå¤šæ¨¡æ€ AI è¯†åˆ«")

    uploaded = st.file_uploader("é€‰æ‹© PDF æ–‡ä»¶", type=["pdf"])
    if not uploaded:
        st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ ä¸€ä¸ª PDF æ–‡ä»¶ä»¥å¯åŠ¨ç®¡é“ã€‚")
        return

    if not api_key:
        st.error("âŒ ç¼ºå°‘ API Keyã€‚")
        return

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded.getvalue())
        tmp_path = tmp.name

    # ============== STEP 1 ==============
    st.markdown("---")
    st.markdown('<div class="step-card"><div class="step-title">âœ… STEP 1 â€” åŠ è½½ PDF<span class="badge badge-ok">load_pdf</span></div></div>', unsafe_allow_html=True)

    result = load_pdf(tmp_path, uploaded.name)
    if not result: return
    doc, meta_report, is_scanned = result

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("ğŸ“„ æ–‡ä»¶å", uploaded.name[:20])
    c2.metric("ğŸ“ å¤§å°", f"{meta_report['file_size_kb']:.1f} KB")
    c3.metric("ğŸ“‘ æ€»é¡µæ•°", meta_report['page_count'])
    c4.metric("ğŸ”¤ é¦–é¡µå­—ç¬¦", meta_report['first_page_char_count'])

    # ============== STEP 2 ==============
    st.markdown("---")
    st.markdown('<div class="step-card"><div class="step-title">ğŸ–¼ï¸ STEP 2 â€” å…¨é¡µæ¸²æŸ“<span class="badge badge-ocr">render_pages</span></div></div>', unsafe_allow_html=True)

    with st.spinner(f"æ­£åœ¨ä»¥ {render_dpi} DPI æ¸²æŸ“ {meta_report['page_count']} é¡µâ€¦"):
        pages = render_pages(doc, dpi=render_dpi)
    st.success(f"âœ… æ¸²æŸ“å®Œæˆï¼Œå…± {len(pages)} å¼ å›¾ç‰‡ã€‚")

    # ============== STEP 3 ==============
    st.markdown("---")
    st.markdown('<div class="step-card"><div class="step-title">ğŸ“¸ STEP 3 â€” æå–åŸç”Ÿä¿ç•™æ’å›¾<span class="badge badge-ok">extract_images</span></div></div>', unsafe_allow_html=True)
    
    with st.spinner("æ­£åœ¨ä» PDF ç»“æ„ä¸­æ·±å±‚æå–åŸå§‹æ’å…¥ä½å›¾â€¦"):
        images_data = extract_images_from_pdf(doc, uploaded.name)
        
    # Now we can safely close doc
    doc.close()

    st.success(f"âœ… æå–å®Œæˆã€‚æˆåŠŸè·å–åˆ° {len(images_data)} å¼ æœ‰æ•ˆåŸç”Ÿå›¾ç‰‡ã€‚")
    if images_data:
        with st.expander(f"âœ¨ åŸç”Ÿå›¾ç‰‡ç”»å»Šï¼ˆ{len(images_data)} å¼ ï¼‰", expanded=False):
            img_cols = st.columns(min(3, len(images_data)))
            for i, img_dict in enumerate(images_data):
                with img_cols[i % 3]:
                    st.image(img_dict["image_bytes"], use_container_width=True)
                    st.caption(f"ç¬¬ {img_dict['page']} é¡µ Â· {img_dict['size_label']} Â· .{img_dict['ext']}")
                    if img_dict.get("caption_context"):
                        with st.expander("ğŸ“ æŸ¥çœ‹ Caption ä¸Šä¸‹æ–‡", expanded=False):
                            st.write(img_dict["caption_context"])

    # ============== STEP 4 ==============
    st.markdown("---")
    st.markdown(f'<div class="step-card"><div class="step-title">ğŸ¤– STEP 4 â€” å¹¶è¡Œå¤šæ¨¡æ€ OCR<span class="badge badge-ocr">{max_workers} å¹¶å‘</span></div></div>', unsafe_allow_html=True)

    ocr_results = parallel_ocr(pages, api_key=api_key, max_workers=max_workers)

    success_count = sum(1 for r in ocr_results if not r["error"])
    combined_md = "\n\n---\n\n".join(
        f"<!-- Page {r['page']} -->\n{r['markdown']}" if r['markdown'] else f"<!-- Page {r['page']} ERROR: {r['error']} -->"
        for r in ocr_results
    )

    with st.expander("ğŸ“„ é€é¡µ OCR ç»“æœæµè§ˆ", expanded=False):
        for r in ocr_results:
            hdr = f"ç¬¬ {r['page']} é¡µ {'âœ…' if not r['error'] else 'âŒ'}"
            with st.expander(hdr, expanded=False):
                if r["error"]: st.error(r["error"])
                else: st.markdown(r["markdown"][:3000])

    # ============== STEP 5 ==============
    st.markdown("---")
    st.markdown('<div class="step-card"><div class="step-title">ğŸ§¹ STEP 5 â€” æ–‡æœ¬æ¸…æ´— + å…¬å¼ç»Ÿè®¡<span class="badge badge-ok">clean_text</span></div></div>', unsafe_allow_html=True)

    cleaned, clean_stats = clean_text(combined_md)

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("æ¸…æ´—åå­—ç¬¦", f"{clean_stats['cleaned_chars']:,}")
    c2.metric("å‹ç¼©ç‡", clean_stats['compression_rate'])
    c3.metric("ğŸ”¢ ç‹¬ç«‹å…¬å¼ $$", clean_stats['block_formulas'])
    c4.metric("ğŸ“ è¡Œå†…å…¬å¼ $", clean_stats['inline_formulas'])

    # ============== STEP 6 ==============
    st.markdown("---")
    st.markdown('<div class="step-card"><div class="step-title">âœ‚ï¸ STEP 6 â€” æ–‡æœ¬åˆ†å—<span class="badge badge-ok">chunk_text</span></div></div>', unsafe_allow_html=True)

    chunks, para_count = chunk_text(cleaned, chunk_size=CHUNK_SIZE)
    c1, c2, c3 = st.columns(3)
    c1.metric("ğŸ“‹ è‡ªç„¶æ®µè½", para_count)
    c2.metric("âœ‚ï¸ åˆ‡åˆ†å—æ•°", len(chunks))

    # ============== æ±‡æ€» ================
    st.markdown("---")
    st.markdown('<div class="step-card"><div class="step-title">ğŸ‰ å…¨éƒ¨æ­¥éª¤æ‰§è¡Œå®Œæ¯• Â· æ±‡æ€»</div></div>', unsafe_allow_html=True)

    s1, s2, s3, s4 = st.columns(4)
    s1.metric("ğŸ“„ é¡µæ•°", f"{len(pages)}")
    s2.metric("ğŸ“¸ åŸç”Ÿæ’å›¾", f"{len(images_data)}")
    s3.metric("ğŸ”¢ å›¾è¡¨å…¬å¼", f"{clean_stats['block_formulas']} å…¬å¼")
    s4.metric("âœ‚ï¸ æœ€ç»ˆåˆ†å—", len(chunks))

    # ä¸‹è½½åŒº
    st.markdown("##### ğŸ“¥ ä¸‹è½½ç»“æœ")
    dl1, dl2, dl3, dl4 = st.columns(4)
    dl1.download_button("Step 4 Â· OCR Markdown", data=combined_md, file_name="step4_ocr_result.md", mime="text/markdown")
    dl2.download_button("Step 5 Â· æ¸…æ´—æ–‡æœ¬", data=cleaned, file_name="step5_cleaned.md", mime="text/markdown")
    if images_data:
        # å¯¼å‡ºå¸¦ caption info çš„å›¾ç‰‡ metadata json
        img_meta = [
            {k: v for k, v in d.items() if k != "image_bytes"}
            for d in images_data
        ]
        
        dl3.download_button(
            "Step 3 Â· å›¾ç‰‡ä¿¡æ¯ (JSON)", 
            data=json.dumps(img_meta, ensure_ascii=False, indent=2), 
            file_name="step3_images_meta.json", mime="application/json"
        )
        dl4.download_button(
            "Step 3 Â· [ç¤ºä¾‹] é¦–å¼ æˆªå›¾", 
            data=images_data[0]["image_bytes"], 
            file_name=f"figure_p{images_data[0]['page']}.{images_data[0]['ext']}", 
            mime=f"image/{images_data[0]['ext']}"
        )

    try:
        os.unlink(tmp_path)
    except OSError:
        pass

if __name__ == "__main__":
    main()
